{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "* Neural network with more than one layer\n",
    "* E.g. images of cats, dogs, trucks, etc.\n",
    " * need to reliably identify\n",
    "* We are dealing with **supervised** learning\n",
    " * learning using label data\n",
    "* $x \\to f(\\theta, x) \\to \\hat{y}$\n",
    " * all of variables are vectors/matrices\n",
    " * $x$ is input data\n",
    "   * can even be images in form [$R_1, G_1, B_1, R_2, G_2, B_2, \\dots$]\n",
    " * $\\theta$ is all of the learnable parameters for the network layer\n",
    "   * weigts\n",
    "   * biases\n",
    " * $\\hat{y}$ is predicted output\n",
    "   * 90% cat, 5% dog, etc.\n",
    "   * does not actually contain class names, only class 0, class 1, etc.\n",
    "   * vector of k classes\n",
    "* $f_{net} = f_3(\\theta_3, f_2(\\theta_2, f_1(\\theta_1, x))) \\to$ a \"3 layer\" neural network\n",
    "* $f = \\sum v_i\\phi(\\vec{w_i}\\cdot \\vec{x_i} + b_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Used to find the needed $\\theta$ weights\n",
    "* Done with backpropogation\n",
    "* We will be looking at Stochastic GD, aka SDG\n",
    "* Proccess is also called **learning**\n",
    " * Thus, **machine learning**\n",
    "$\\newcommand{\\Lagr}{\\mathcal{L}}$\n",
    "* Need to define learning with a **loss function** $\\Lagr(\\hat{y}, y)$\n",
    " * good prediction $\\to$ low $\\Lagr$\n",
    " * bad prediction $\\to$ high $\\Lagr$\n",
    " * $\\Lagr$ **must be** continuous/differentiable (if we want to successfully learn)\n",
    "* $x \\to f(\\theta, x) \\to \\hat{y} \\to \\Lagr(\\hat{y}, y)$\n",
    "* Can plot the relationship of $\\theta$ and $\\Lagr$ (make both of them scalars for simplicity)\n",
    " * Slope at any $\\theta$ is $\\dfrac{d\\Lagr}{d\\theta}\\biggr\\rvert_{\\theta}$\n",
    " * Can go down slope\n",
    " * $\\theta_{new} = \\theta_{old} - \\mu\\dfrac{d\\Lagr}{d\\theta}\\biggr\\rvert_{\\theta=\\theta_{old}}$\n",
    "* SDG finds a (laziest) way to optimize $\\to$ good at shortcuts\n",
    "* Only as good as your data\n",
    "* Can't directly compute minimum\n",
    " * hard - many parameters\n",
    " * useless - only good for the specific input point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "* Outputs are numbers (persumably between 0 and 1)\n",
    "* Represent a value\n",
    "* Examples:\n",
    " * x-position of faceof a person\n",
    " * value of a sin(x) function\n",
    " * expected lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univ. Function Approximator\n",
    "* Regression\n",
    "* $x \\to f_{NN}(\\theta, x) \\to \\hat y$\n",
    "* 2-layer \"NN\"\n",
    " * 1-N-1 network\n",
    " * Multiplications:\n",
    "   * (M, 1) x (1, N) x (N, 1) = (M, 1)\n",
    "   * (M, 1) $\\to$ (M, N) $\\to$ (M, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "* Outputs represent classes\n",
    " * Higher number = more likelyhood\n",
    "* Use softmax and cross_entropy\n",
    "* Examples:\n",
    " * Cats/dogs\n",
    " * Survived/died in the Titanic incident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tendril classificator\n",
    "* Classification\n",
    "* 2-layer \"NN\"\n",
    " * 2-N-3\n",
    " * Multiplications:\n",
    "   * (M, 2) x (2, N) x (N, 3) = (M, 3)\n",
    "   * (M, 2) $\\to$ (M, N) $\\to$ (M, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "* L1 loss: $\\sum \\limits_{i=1}^N|\\hat {y_i} - y_i|$\n",
    "* L2 loss: $\\sum \\limits_{i=1}^N(\\hat {y_i} - y_i)^2$ (similar to MSE)\n",
    "* cross-entropy: $âˆ’\\sum \\limits_{i=1}^N y_i\\log(\\hat {y_i})$ (goes well with softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "* Can think of previous examples as 2-layer neural networks\n",
    "* Each neuron performs $\\phi(\\vec x \\cdot \\vec{w_i} + b_i)$\n",
    "* Dense layers\n",
    " * all neurons see all neurons from their preceding layer\n",
    "* Can have way more than 2 layers!\n",
    " * Example with 2-12-6-3\n",
    "   * (M, 2) x (2, 12) x (12, 6) x (6, 3) = (M, 3)\n",
    "   *   $x$  x  $W_1$  x  $W_2$  x  $W_3$ = $\\hat y$\n",
    "* Can have various activation functions\n",
    " * using all linear activation functions causes network to collapse\n",
    " * reLu allows layers to ignore some inputs\n",
    "   * still learn fast while strongly wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification\n",
    "* We aren't bound to a small dimensionality of data\n",
    "* Can use enough dimensions for an entire image!!!\n",
    " * cifar-10: 32x32x3 = 3074 dimensions!!!\n",
    " * can flatten the image to make each a vector in $\\mathbb{R}^{3074}$ space\n",
    "* **But**, there's a caveat...\n",
    " * Having high dimensionality causes many connections\n",
    " * Many connections cause the netowrk to learn worse\n",
    " * So, what do we do?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before:\n",
    " * $x \\cdot W \\to$ (1, 3072) x (3072, 10) = (1, 10)\n",
    " * equiv. to $(\\vec x \\cdot \\vec{w_0}, \\vec x \\cdot \\vec{w_1}, \\vec x \\cdot \\vec{w_2}, \\dots)$\n",
    "   * Dot product describes similarity of vectors\n",
    "   * $\\vec x \\cdot \\vec y = \\lVert\\vec x\\rVert\\lVert\\vec y\\rVert\\cos\\theta$\n",
    "   * In a 1-layer, weights would represent a class (i.e. cat, dog) mask\n",
    "   * How much of it is a cat/dog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "* technically, autocorrelations, but eh..\n",
    "* Move **kernel**\n",
    "\\begin{bmatrix}\n",
    "    3&8 \\\\\n",
    "    1&0 \\\\\n",
    "\\end{bmatrix}\n",
    "* Over **image**\n",
    "\\begin{bmatrix}\n",
    "    0&2&3&6&9 \\\\\n",
    "    8&4&1&6&2 \\\\\n",
    "    1&0&2&2&8 \\\\\n",
    "    8&6&0&6&3 \\\\\n",
    "    1&5&4&9&1 \\\\\n",
    "\\end{bmatrix}\n",
    "* Kernel at (0,0)\n",
    " * $3\\cdot0 + 8\\cdot2 + 1\\cdot8 + 0\\cdot4 = 24$\n",
    "* Kernel at (0,1)\n",
    " * $3\\cdot2 + 8\\cdot3 + 1\\cdot4 + 0\\cdot1 = 34$ \n",
    "* Kernel at (1,0)\n",
    " * $3\\cdot8 + 8\\cdot4 + 1\\cdot1 + 0\\cdot0 = 57$ \n",
    "* Kernel at (1,1)\n",
    " * $3\\cdot4 + 8\\cdot1 + 1\\cdot0 + 0\\cdot2 = 20$\n",
    "* $\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24, 34, 58, 96],\n",
       "       [57, 20, 53, 36],\n",
       "       [11, 22, 22, 76],\n",
       "       [73, 23, 52, 51]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mygrad.nnet.layers.utils import sliding_window_view\n",
    "\n",
    "img = np.array([[0,2,3,6,9],\n",
    "                [8,4,1,6,2],\n",
    "                [1,0,2,2,8],\n",
    "                [8,6,0,6,3],\n",
    "                [1,5,4,9,1]])\n",
    "kernel = np.array([[3, 8],\n",
    "                   [1, 0]])\n",
    "views = sliding_window_view(img, window_shape=(2, 2), step=(1, 1))\n",
    "np.sum(views * kernel, axis=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can also skip over some with stride\n",
    "* Ouput shape: $\\dfrac{x-w}{s}+1$\n",
    " * has to be an integer\n",
    "* Stride and kernel can also be rectangular\n",
    "* MyGrad can preform convolutions in **N dimensions!**\n",
    "* $(N, C, H, W) \\circledast (F, C, H_f, W_f) \\to (N, F, H', W')$\n",
    " * $N$ is number of images\n",
    " * $C$ is number of channels\n",
    " * $F$ is number of different kernels\n",
    " * $H$ is image height\n",
    " * $W$ is image width\n",
    " * $H_f$ is kernel height\n",
    " * $W_f$ is kernel width\n",
    " * $H' = \\dfrac{H-H_f}{s}+1$\n",
    " * $W' = \\dfrac{W-W_f}{s}+1$\n",
    " * Each kernel has C channels $\\to$ channel dimension collapses\n",
    " * Replaced by F dimension\n",
    "* F dimension essentially replaces the number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* Mathematically, dependent on convoled parameters\n",
    "* Practically, slightly wiggle parameters and observe changes\n",
    "* Results line up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpooling\n",
    "* Same as convolutions, but returns max\n",
    "* One value over each channel\n",
    "* Typically done with strides, so dimensionality is reduced\n",
    " * i.e. a 2x2 maxpooling layer will output $\\dfrac{x-2}{2}+1 = \\dfrac{x}{2}$\n",
    "* Typically follow convolutions\n",
    " * don't have to, can be entirely replaced with strided convolutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
