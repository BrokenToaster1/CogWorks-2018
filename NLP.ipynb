{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "| | Humanly | Rationally |\n",
    "| --- | --- | --- |\n",
    "| Thinking | \"cognitive models\" | \"laws of thought\" |\n",
    "| Acting | \"Turing Test\" | \"rational agent\" |\n",
    "\n",
    "* **Humanly** - Imitating nature\n",
    " * like planes imitating birds\n",
    " * did not work, switched to physics\n",
    "* **Acting** - behaving with respect to outside observers\n",
    " * can be text output or action output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline\n",
    "* 1997: Deep Blue\n",
    "* 2004: NASA Mars Rover\n",
    "* 2007: DARPA Urban Challenge\n",
    "* 2009: \"Adam\"\n",
    "* 2011: Siri | Watson\n",
    "* 2014: Eugene | Alexa\n",
    "* 2015: Atari\n",
    "* 2016: AlphaGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets\n",
    "* **AI**: symbolic\n",
    " * expert systems\n",
    " * **ML**: ability to learn from experience\n",
    "   * SVMs, decision trees\n",
    "   * **DL**: Learn hierarhical representation\n",
    "     * CNNs, RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "* What can NLP be used for?\n",
    " * Article summarization\n",
    " * Terrorist detection\n",
    " * Targeted ads\n",
    " * Personal assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language\n",
    "* Aquiring and using complex systems of information\n",
    "* Most basic level: sound pressure modulations\n",
    "* More processed: text\n",
    "* Why use processed?\n",
    " * preprocessing done\n",
    " * higher information density\n",
    " * sometimes generated that way\n",
    "* Can attach higher-level info\n",
    " * parts of speech\n",
    " * prepositionsl, noun, etc. phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling\n",
    "* Computing the **probability** of seeing characters or phrases\n",
    " * $P(w_1, w_2, \\dots w_T)$\n",
    " * \"the black cat\" more common than \"the cat black\"\n",
    "* Based on previous words\n",
    " * $P(w_i|w_1,\\dots w_{i-1}) \\approx P(w_i|w_{i-(n-1)},\\dots w_{i-1})$\n",
    " * trigram: $P(w_i|w_{i-2}, w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple language model\n",
    "* History (size n) of characters maps to predicted\n",
    "* Predict based on probabilities\n",
    " * thus, not as likely to loop\n",
    "* Use a dictionary when analyzing text\n",
    " * E.g.:\n",
    "   * \"ca\" $\\to$ (c, 0.5), (o, 0.5)\n",
    "   * \"ac\" $\\to$ (a, 1.0)\n",
    "   * $\\dots$\n",
    "\n",
    "#### Application - text immitator:\n",
    "* Generate Shakespeare\n",
    " * or any other text, for that matter\n",
    "* Combine several songs into one file, generate an amalgamation of them\n",
    "\n",
    "#### Problems:\n",
    "* Does not remember well\n",
    " * nonsensical\n",
    " * tends to copy text with large windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "* How do we analyze and compare documents?\n",
    "* Can count number of words in each and their term-frequencies - **TF** \n",
    "* What about presence and absence of words?\n",
    " * can use a common **bag of words**\n",
    "* Need to only select the most common\n",
    "* Need to ignore **stopwords**\n",
    "* Can ignore words that have a similar frequency in all documents\n",
    " * compute inverse document frequency - **IDF**\n",
    " * $\\log_{10}\\dfrac{N}{n_f}$\n",
    "* That leads us to a final measure of **TF-IDF**\n",
    " * multiply TF by IDF\n",
    "* Applications?\n",
    " * can compute distance using dot product\n",
    " * can use as higher-level featues in other models\n",
    "\n",
    "#### Application - IMDB Reviews:\n",
    "* Take IMDB reviews, classify as positive/negative\n",
    "* Create matrix of documents to words\n",
    "* Evaluate on TF-IDF\n",
    "* Use a single-layer network\n",
    "\n",
    "#### Problems:\n",
    "* Eliminating stopwords removes some information\n",
    " * can no longer detect negation\n",
    "* Can not detect the order of words\n",
    "* Large dimensionality of data\n",
    "* Can not relate words that are similar to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "* Way to reduce the dimensionality of words\n",
    " * Can use the new reduced features in other methods and networks\n",
    "* Can learn semantics\n",
    " * can do arithmetic!\n",
    " * king - man + woman $\\approx$ queen\n",
    "* Can detect synonyms or similar words\n",
    " * dog and puppy\n",
    " * happy and glad\n",
    "* Makes our history much easier\n",
    " * did not have to explicitly see example to determine what's next\n",
    " * yet, our dictionary is no longer discrete\n",
    "* Some versions:\n",
    " * word2vec\n",
    " * GloVe\n",
    "* Technique we'll be using: **SVD**\n",
    " * use sliding windows for proximity\n",
    " * create a $N^2$ table of correspondence\n",
    "\n",
    "#### Application - word embedding calculator:\n",
    "* Analyze wikipedia corpus\n",
    " * or any other corpus, for that matter\n",
    "* Do vector arithmetic to solve analogies!\n",
    "\n",
    "#### Application - tweet classifier:\n",
    "* Simialr to IMDB (positive or negative), but now infer semantics\n",
    "* Use a CNN\n",
    " * 1-D convolutions\n",
    "   * $(N, D, S) \\times (F, D, S_f) \\to (N, F, S')$\n",
    " * 1-neuron output\n",
    " * x $\\to$ conv $\\to$ relu $\\to$ global pool $\\to$ dense $\\to$ relu $\\to$ dense $\\to$ sigmoid\n",
    " * $(N, D, S) \\to (N, F, S') \\to (N, F, 1) \\to (N, F) \\to (N, 200) \\to (N, 1)$\n",
    "* Can have different length S - pad with zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "* Variance:\n",
    " * represents distance of one point\n",
    " * $var(1) = (x_{1,i} - \\bar x_i)^2$\n",
    " * $S = \\dfrac{XX^T}{N-1}$\n",
    "   * normalized over whole distribution with N-1\n",
    " * $S_{11} = var(1)$\n",
    "* Covariance:\n",
    " * represents similaritty of two points\n",
    " * $S_{12} = cov(1, 2)$\n",
    "* $S = U\\Sigma U^{-1}$\n",
    " * $U = $ orthogonal matrix\n",
    "   * columns $U_i\\cdot U_j = 0$\n",
    "   * $\\lVert U_c\\rVert = 1$\n",
    " * $\\Sigma = $ diagonal matrix\n",
    " * operations $U$ and $U^{-1}$ **change perspective**\n",
    "* What are we trying to do?\n",
    " * Find a $U$-matrix sugh that..\n",
    "   * $\\Sigma$ is a diagonal matrix\n",
    "     * covariances are 0\n",
    "     * variables are not dependent on each other\n",
    "   * Latter values along the diagonal are smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "* Apply the found $U$ to actual data\n",
    " * $Y = U^TX$\n",
    "* Latter values along the diagonal are smaller\n",
    " * remove to reduce dimensionality\n",
    " * extract the remaining diagonal to a final vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation: Application of failure\n",
    "* What if we want to compute number of 1's in a sequence and return 0/1 based on even/odd?\n",
    "* Both NNs and CNNs do not learn\n",
    " * only learn through memorizing\n",
    " * massive overfitting\n",
    " * testing accuracy stays bad\n",
    "\n",
    "#### Solution?\n",
    "* How do humans approach it?\n",
    " * method 1: count sum\n",
    " * method 2: switch between 1 and 0\n",
    "* Thus, need to input numbers **sequentially**\n",
    "\n",
    "Introducing...\n",
    "# RNNs\n",
    "* Typical 2-layer: $y\\\\\n",
    "\\uparrow\\\\\n",
    "h\\\\\n",
    "\\uparrow\\\\\n",
    "x$\n",
    "* Neurons can take their own input\n",
    " * from previous iteration\n",
    " * $h_{n-1} \\to h_n$\n",
    " * can unroll the representation\n",
    " * $y_1\\quad\\:\\,y_2\\quad\\:\\,y_3\\qquad\\quad\\: y_t\\\\\n",
    " \\uparrow\\quad\\ \\uparrow\\quad\\ \\uparrow\\qquad\\quad\\;\\;\\uparrow\\\\\n",
    " h_1\\to h_2\\to h_3-\\dots\\to h_t\\\\\n",
    " \\uparrow\\quad\\ \\uparrow\\quad\\ \\uparrow\\qquad\\quad\\;\\;\\uparrow\\\\\n",
    " x_0\\quad\\:\\,x_1\\quad\\:\\,x_2\\qquad\\quad\\: x_{t-1}$\n",
    "* Essentially, propagating through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're propagating through time, we also need to be...\n",
    "### Backpropagating through time\n",
    "* $h_t = f(x_{t-1}W+h_{t-1}V)$\n",
    " * Let $\\hat x = x_{t-1}W+h_{t-1}V$\n",
    " * $W$ shape (C, D)\n",
    " * $V$ shape (D, D)\n",
    "* $(x_0, x_1,\\dots x_T) \\to RNN(x;W,V) \\to \\hat y = (y_1, y_2,\\dots y_T)$\n",
    "$\\newcommand{\\Lagr}{\\mathcal{L}}$\n",
    "* $\\dfrac{\\partial\\Lagr}{\\partial V}=\\dfrac{d\\Lagr}{dy_t}\\dfrac{dy_t}{dh_t}\\dfrac{dh_t}{dV}$\n",
    "* $\\dfrac{dh_t}{dV}=\\dfrac{df}{d\\hat x}\\dfrac{d\\hat x}{dV}$\n",
    "* $\\dfrac{d\\hat x}{dV}=\\dfrac{d(h_{t-1}V)}{dV}=h_{t-1}(\\dfrac{dV}{dV})+(\\dfrac{dh_{t-1}}{dV})V=h_{t-1}+(\\dfrac{dh_{t-1}}{dV})V$\n",
    "* $h_t = Vh_{t-1}$\n",
    "* $\\dfrac{\\partial h_t}{\\partial V}=h_{t-1}$\n",
    "* $\\dfrac{dh_t}{dV}=\\dfrac{\\partial h_t}{\\partial V}+\\dfrac{\\partial h_t}{\\partial h_{t-1}}\\dfrac{\\partial h_{t-1}}{\\partial V}+\\dfrac{\\partial h_t}{\\partial h_{t-1}}\\dfrac{\\partial h_{t-1}}{\\partial h_{t-2}}\\dfrac{\\partial h_{t-2}}{\\partial V}+\\dots$\n",
    "* Important distinction:\n",
    " * $\\dfrac{dh_t}{dV}$ is a total derivative\n",
    " * $\\dfrac{\\partial h_t}{\\partial V}$ is a partial derivative\n",
    " * difference: partial does not go beyond symbolic representation\n",
    " * example:\n",
    "   * $z = t^2$\n",
    "   * $f = z\\cdot t$\n",
    "   * Thus, $\\dfrac{\\partial f}{\\partial t} = z = t^2$\n",
    "   * and $\\dfrac{df}{dt} = \\dfrac{d}{dt}t^3 = 3t^2$\n",
    "   * $\\dfrac{df}{dt} = \\dfrac{\\partial f}{\\partial t}\\dfrac{dt}{dt} + \\dfrac{\\partial f}{\\partial z}\\dfrac{dz}{dt} = t^2 + t\\cdot 2t = 3t^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application - even/odd:\n",
    "* Simple RNN performs much better than a NN with many more parameters\n",
    "* So simple, we can even hand-pick parameters!\n",
    "\n",
    "#### Application - repeated sequences:\n",
    "* Check if first half of a sequence matches the rest\n",
    "* Use the same simple RNN\n",
    "* Can use batches, but samples have to be of equal length among batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
