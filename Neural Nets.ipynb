{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univ. Function Approximator\n",
    "* Regression\n",
    "* $x \\to f_{NN}(\\theta, x) \\to \\hat y$\n",
    "* 2-layer NN\n",
    " * 1-N-1 network\n",
    " * Multiplications:\n",
    "   * (M, 1) x (1, N) x (N, 1) = (M, 1)\n",
    "   * (M, 1) $\\to$ (M, N) $\\to$ (M, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tendril classificator\n",
    "* Classification\n",
    "* 2-layer NN\n",
    " * 2-N-3\n",
    " * Multiplications:\n",
    "   * (M, 2) x (2, N) x (N, 3) = (M, 3)\n",
    "   * (M, 2) $\\to$ (M, N) $\\to$ (M, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "* Can think of previous examples as 2-layer neural networks\n",
    "* Each neuron performs $\\phi(\\vec x \\cdot \\vec{w_i} + b_i)$\n",
    "* Dense layers\n",
    " * all neurons see all neurons from their preceding layer\n",
    "* Can have way more than 2 layers!\n",
    " * Example with 2-12-6-3\n",
    "   * (M, 2) x (2, 12) x (12, 6) x (6, 3) = (M, 3)\n",
    "   *   $x$  x  $W_1$  x  $W_2$  x  $W_3$ = $\\hat y$\n",
    "* Can have various activation functions\n",
    " * using all linear activation functions causes network to collapse\n",
    " * reLu allows layers to ignore some inputs\n",
    "   * still learn fast while strongly wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
