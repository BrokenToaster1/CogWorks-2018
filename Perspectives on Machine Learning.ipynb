{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [What a Deep Neural Network thinks about your #selfie](http://karpathy.github.io/2015/10/25/selfie/)\n",
    "* By Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNets\n",
    "* Recognize various things in photo\n",
    "* Developed in 1980's\n",
    "* Ignored until 2012\n",
    "* Performance:\n",
    " * simple\n",
    " * fast\n",
    " * accurate\n",
    "* Propagates filters on an image\n",
    "* Similar to a child learning features to look for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ConvNets and selfies\n",
    "* Can be used to score\n",
    "* Score can be used to find ultimate crop\n",
    "* Twitter @deepselfie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Basics of Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "* *m*x*n* means *m* rows and *n* columns (height *m*, width *n*)\n",
    "* *m*x*n* consistent with numpy's *shape*\n",
    "* **Multiplication**: dot-product of vectors (1st: horiz, 2nd: vert)\n",
    "* $C = AB$\n",
    "* $C_{i,j} = a_i\\cdot b_j$\n",
    "* $(m, n) = (m, k)\\cdot(k, n)$\n",
    "* NOT commutative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  3,  2,  9],\n",
       "       [26,  7, 10, 24]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "[2, 0, 1],\n",
    "[3, 1, 2]])\n",
    "\n",
    "B = np.array([\n",
    "[8, 1, 1, 3],\n",
    "[0, 2, 7, 9],\n",
    "[1, 1, 0, 3],])\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "* Single column (*n*x1) or row (1x*n*)\n",
    "* **Dot-product**: sum of corresponding products (same size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6, 7, 8])\n",
    "\n",
    "# doesn't matter if row or column vectors\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* By Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "* **Recurrent Neural Networks**\n",
    "* Traditional ML constrained by fixed I/O dimensions\n",
    "* RNNs bypass said limitation; work with sequential data\n",
    "* Variations\n",
    " 1. fixed->fixed\n",
    " 1. fixed->sequence\n",
    " 1. sequence->fixed\n",
    " 1. sequence->sequence\n",
    " 1. sequence->sequence (synced)\n",
    "* Turing-complete (can simulate any program)\n",
    "* Can also process fixed-size data sequentially\n",
    "* Single function step\n",
    " * vector x -> RNN -> vector y\n",
    " * y = rnn.step(x)\n",
    " * $h_t=tanh(W_{hh}h_{tâˆ’1}+W_{xh}x_t)$\n",
    " * $y_t=W_{hy}h_t$\n",
    "* Also influenced by past steps; i.e. memory\n",
    "* Add layers by stacking RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "* **Long Short-Term Memory** Networks\n",
    "* Subclass of RNNs\n",
    "* \"Forget\" things, better remember more important ones\n",
    "* $h_t=...$ gets more complicated\n",
    "* Temperature parameter (0, 1]\n",
    " * near 0: representative and certain, but repetative\n",
    " * near 1: diverse, but prone to errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Applications\n",
    "* Proof-of-concept applications\n",
    " * Writing\n",
    " * Code\n",
    " * Research Papers\n",
    " * Baby names\n",
    " * etc.\n",
    "* Real-world applications\n",
    " * Translation\n",
    " * Speech to text\n",
    " * Handwritten text\n",
    " * Image/video classification & captioning\n",
    " * Memories and attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Learning Proccess\n",
    "* Learns simple concepts, then harder ones\n",
    " * Spaces\n",
    " * Commas before spaces\n",
    " * Periods at sentence end\n",
    " * Common, short words\n",
    " * Longer words\n",
    " * Quotation formatting, advanced punctuation\n",
    " * Proper spelling, names\n",
    "* Higly certain on \"http:www\", etc.\n",
    "* Hidden neurons react to certain stimuli and remember certain things\n",
    " * Inside containers (quotations, parentheses, markdown, urls), to close them off later\n",
    " * Time since container start or line break\n",
    " * Remaining ~95% non-interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "* By Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* Recent progress in CV and RL mainly driven by compute/data/infrastructure, not algorithms\n",
    "* Some algoritms:\n",
    " * DQN (Deep Q-network)\n",
    "   * a.k.a. Q-learning\n",
    " * PG (Policy Gradients)\n",
    "   * Direct optimization of reward\n",
    "* Structure\n",
    " * Input (Pixels from Pong)\n",
    " * Action (Move paddle up/down)\n",
    " * Reward (Change in game score)\n",
    " * NN trained on Input->Action\n",
    "   * Side note: feed in multiple frames\n",
    "* Credit assignment problem\n",
    " * What caused score change and when?\n",
    " * How to optimize weights?\n",
    " * SOLUTION: Wait until reward, train each step with its action output * reward\n",
    "   * Output not a distribution: all 0, performed action 1\n",
    "   * Positive reward encourages perfomed actions, negative discourages\n",
    " * Can now solve any problem\n",
    "* Like supervised, but on a changing dataset\n",
    "* Alternative: discounted reward\n",
    " * Reward based on future states, but exponentially decreasing with time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "* Special case of *function gradient estimator*s\n",
    "* Gradient to optimize parameters to increase reward\n",
    "* Loss: $\\sum_i A_i \\log p(y_i \\mid x_i)$\n",
    " * $E_x [f(x)] = \\sum_x p(x) f(x)$ (similar thing, rewritten)\n",
    " * $\\nabla_{\\theta}E_x [f(x)] = E_x[f(x) \\nabla_{\\theta} \\log p(x) ]$ (policy gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL vs. humans\n",
    "* No prior knowledge of reward system\n",
    "* No intuitive prior knowledge\n",
    "* Has to 'stumble upon' a strategy to discover it\n",
    "* Have to experience reward often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-differentiable computation\n",
    "* Ability of choice from actions creates non-differentiable functions\n",
    "* PGs can update parameters related to making such choices\n",
    "* SOLUTION:\n",
    " * Update differentiable with gradient descent\n",
    " * Update non-differentiable with PG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainiable memory\n",
    "* Some networks can read/write to memory (to increase performance)\n",
    "* Only soft I/O differentiable (also very slow)\n",
    "* Hard I/O non-differentiable\n",
    "* SOLUTION: Policy Gradients\n",
    "* Comes with PG drawbacks\n",
    " * Mainly, has to stumble onto solutions in vast memory space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future research\n",
    "* Unsupervised generative models and program induction (abstract model building)\n",
    "* Reward modeled by evaluator network\n",
    "* Human supervision + apprenticeship learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG in practice\n",
    "* Need many samples\n",
    "* Takes a long time\n",
    "* Check baseline: cross-entropy method\n",
    "* Variation: TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [The state of Computer Vision and AI: we are really, really far away](http://karpathy.github.io/2012/10/22/state-of-computer-vision/)\n",
    "* By Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring information from images\n",
    "* Many potential inferences from one image\n",
    " * Humans can do in a short glance\n",
    "* CORE ISSUE: deriving information from 2D pixel values\n",
    " * Scene information: structure, objects, people\n",
    " * Intuitive information: physics, how scales work\n",
    " * Social inferences: people's intents, thoughts, reactions\n",
    " * Etc.\n",
    "* AI and CV currently far from being able to retrieve & synthesize this information\n",
    " * Not simply a matter of data/resources/tricky algorithms\n",
    " * Missing fundamental pieces of the puzzle\n",
    " * Long ways off, but a matter of continuous small improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [DeepMind and Blizzard to release StarCraft II as an AI research environment](https://deepmind.com/blog/deepmind-and-blizzard-release-starcraft-ii-ai-research-environment/)\n",
    "* By Oriol Vinyals (DeepMind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarCraft and AI\n",
    "* Collaboration between Blizzard and DeepMind to open up StarCraft II to researchers\n",
    "* Games are a perfect environment for AI development\n",
    " * Complex problem solving without specific instructions\n",
    "* StarCraft connects AI research to the messiness of the real-world\n",
    " * In-game economy\n",
    " * Partially observable environment & scouting\n",
    "   * Remembering & recalling information\n",
    " * Long-term planning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
